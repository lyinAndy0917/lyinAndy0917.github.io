[{"content":"","date":"0001-01-01T00:00:00Z","image":"https://lyinandy0917.github.io/p/control/robo_hu_c37ce0f68831d44b.png","permalink":"https://lyinandy0917.github.io/p/control/","title":"Control"},{"content":"Convolution What is convolution? Convolution is a mathematical operation that combines two signals to produce a third signal. Understanding: 卷积 何为“卷“： 一个信号在另一个信号上作反向平移 何为“积“： 两个信号的乘积 $$y(t) = \\int_{-\\infty}^{\\infty} f(\\tau)g(t-\\tau)d\\tau $$In terms of Matrix Lets say we have a 3x3 matrix A as a kernel And We have a 5x5 matrix B as an image With this kernel, we can slide it over the image At each position, we multiply the kernel and then you would get a new matrix C which has the dimension of 4x4 But now what about the edges? We can pad the image with zeros to make the output matrix the same size as the input matrix This is called zero-padding Image processing f is the image g is the kernel y is the output image In this application, Convolution is like telling how the near points would have an effect on the current point In feature extraction, the kernel is the feature detector. It would compare the kernel with the image and by from the result you could tell how relevant the feature is to that part of the image. ","date":"0001-01-01T00:00:00Z","permalink":"https://lyinandy0917.github.io/p/control/convolution/","title":"Convolution"},{"content":"Discretization Motivation Q: Why do we need to discretize a continuous system? A: Because computers are discrete. They can only process discrete data. Discretized systems have some benefits that continuous systems don\u0026rsquo;t have: Robustness: Discretized systems are more robust to noise and disturbances. Explicit Form $X_{k+1} = f(X_k, U_k)$ Q: Why is it called explicit form?\nA: Because the next state is explicitly determined by the current state and the control input.\n$f$ is the system dynamics.\nMethods Simplest Discretization - Forward Euler $$ h = \\text{time step} $$ This method takes the slope at the current point and uses it to predict the next point. This method always overshoots.\nThis method is so bad because this method adds energy to the system. The energy is not conserved. Should not use this method.\nRunge-Kutta Method In most case, we use the Runge-Kutta method to discretize the system.\n1 2 3 4 5 6 def runge_kutta(f, x, u, h): k1 = f(x, u) k2 = f(x + h/2 * k1, u) k3 = f(x + h/2 * k2, u) k4 = f(x + h * k3, u) return x + h/6 * (k1 + 2*k2 + 2*k3 + k4) Implicit Form Q: Why is it called implicit form?\nA: Because the next state is implicitly determined by the current state and the control input. The equation is not directly solvable, and it would contain the next state.\n$f_{explicit}(X_{k+1}, X_k, U_k) = 0$\nSince the equation is not directly solvable, we need to use numerical methods to solve it. Q: How do we know if we have come to a solution? A: We would define a residual function. If the residual function is zero (or very close to zero), we have come to a solution.\nMethods Backward Euler $$residual = x_k + \\Delta t \\cdot \\dot{x}_{k+1} - x_{k+1} = 0 \\quad \\quad \\text{Backward Euler}$$\nQ: How is this different from the forward Euler? A: In forward euler, we have the dynamics $f(x_k, u_k)$ at the current time step. In backward euler, we would make use of the dynamics $f(x_{k+1}, u_k)$ at the next time step.\nIn this method, the error is always proportional to the time step.\nQ: Why is this method better than the forward Euler? A: While the forward Euler method always overshoots, the backward Euler method always undershoots. The backward Euler method is more stable than the forward Euler method.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 function implicit_integrator_solve(params::NamedTuple, dynamics::Function, implicit_integrator::Function, x1::Vector, dt::Real;tol = 1e-13, max_iters = 10)::Vector # initialize guess x2 = 1*x1 for i = 1:max_iters residual = implicit_integrator(params, dynamics, x1, x2, dt) # TODO: return x2 when the norm of the residual is below tol if norm(residual) \u0026lt; tol return x2 end # Newton\u0026#39;s method J = FD.jacobian(x -\u0026gt; implicit_integrator(params, dynamics, x1, x, dt), x2) x2 = x2 - J\\residual end error(\u0026#34;implicit integrator solve failed\u0026#34;) end Explanation:\nThis function takes in the parameters, the dynamics, the implicit integrator (backward euler, RK4), the initial state, the time step, the tolerance, and the maximum number of iterations.\nQ: What does the maximum number of iterations affects?\nA: In my opinion, with a easy solution, the result would converge quickly. However, with a more complex system, the result would take more iterations to converge.\nThe function initializes the guess of the next state x2 to be the same as the current state x1.\nAt each iteration, it would check the residual of the implicit integrator. If the residual is below the tolerance, it would return the next state x2. And this is the solution of that particular time step.\nQuick Recap on Newton\u0026rsquo;s Method:\nNewton\u0026rsquo;s method is an iterative method to find the root of a function. It uses the tangent line to approximate the direction of the convergence and eventually find the root. The formula is $x_{n+1} = x_n - \\frac{f(x_n)}{f\u0026rsquo;(x_n)}$ The method would converge to the root if the initial guess is close to the root and the function is well-behaved. Potential Problems you can\u0026rsquo;t control how far the convergence moves at each time step because it only depends on the function. To help with this, could ultilize the line search method to find the optimal step size. $$ x_{n+1} = x_n - \\alpha \\frac{f(x_n)}{f'(x_n)}$$ The $\\alpha$ is the step size. It could be found by the line search method. Simulation To simulate the whole system/curve.\n1 2 3 4 5 6 7 8 9 10 11 12 13 function simulate_implicit(params::NamedTuple,dynamics::Function,implicit_integrator::Function,x0::Vector,dt::Real,tf::Real; tol = 1e-13) t_vec = 0:dt:tf N = length(t_vec) X = [zeros(length(x0)) for i = 1:N] X[1] = x0 # TODO: do a forward simulation with the selected implicit integrator # hint: use your `implicit_integrator_solve` function for i in 2:N X[i] = implicit_integrator_solve(params, dynamics, implicit_integrator, X[i-1], dt) end return X end Now this would simulate each point in the curve. The implicit_integrator_solve function would be used to solve the implicit equation at each time step. Keep in mind this is still discrete.\n","date":"0001-01-01T00:00:00Z","permalink":"https://lyinandy0917.github.io/p/discretization/","title":"Discretization"},{"content":"Controllability Observability Stability with a control gain K, how to check stability? Eigenvalues of the closed-loop system get the closed-loop system matrix $A_{cl} = A - BK$ check the eigenvalues of $A_{cl}$ if the system is in continuous time, check the real part of the eigenvalues. If all the real parts are negative, the system is stable. if the system is in discrete time, check the magnitude of the eigenvalues. If all the magnitudes are less than 1, the system is stable. Lyapunov function find a positive definite function V(x) that satisfies the following conditions $V(0) = 0$ $V(x) \u0026gt; 0$ for all $x \\neq 0$ $\\dot{V}(x) \u0026lt; 0$ for all $x \\neq 0$ if such a function exists, the system is stable Routh-Hurwitz criterion check the coefficients of the characteristic polynomial. Characteristic polynomial is the determinant of $sI - A_{cl}$ if all the coefficients are positive, the system is stable if any of the coefficients are negative, the system is unstable BIBO stability check the transfer function of the system if the transfer function is stable, the system is stable. The transfer function is stable if the poles are in the left half of the complex plane Additional Information what is the difference between Lyapunove stable, BIBO stable, norm stable, and asymptotically stable? Lyapunov stable: the system is stable if the system is bounded. BIBO stable: the system is stable if the input is bounded norm stable: the system is stable if the norm of the system is bounded asymptotically stable: the system is stable if the system converges to a point what does bounded mean? it means the system does not go to infinity. ","date":"0001-01-01T00:00:00Z","permalink":"https://lyinandy0917.github.io/p/foundation/","title":"Foundation"},{"content":"FreeRTOS Q: Why would we need an RTOS?\nA: Because we want to run multiple tasks concurrently.\nQ: What is an RTOS?\nA: An RTOS is an operating system that is designed to handle real-time tasks.\nExample we would create task and a queue. The task would send a message to the queue and the other task would receive the message from the queue.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 For instance ```c QueueHandle_t xQueue; void setup() { serial.begin(9600); xQueue = xQueueCreate(10, sizeof(int)); xTaskCreate(task1, \u0026#34;Task1\u0026#34;, 100, NULL, 1, NULL); xTaskCreate(task2, \u0026#34;Task2\u0026#34;, 100, NULL, 1, NULL); } void loop() { vTaskStartScheduler(); } void LedBlink() { unsigned char message; while(1) { xQueueReceive(xQueue, \u0026amp;message, portMAX_DELAY); for i \u0026lt;message { digitalWrite(13, HIGH); delay(100); digitalWrite(13, LOW); delay(100); } } } void taskReceive(void *pvParameters) { unsigned char message; while(1) { if Serial.available() { int command = Serial.read(); if command == \u0026#39;A\u0026#39; { message = 5; xQueueSend(xQueue, \u0026amp;message, 0); } else if command == \u0026#39;B\u0026#39; { message = 10; xQueueSend(xQueue, \u0026amp;message, 0); } } } } ","date":"0001-01-01T00:00:00Z","permalink":"https://lyinandy0917.github.io/p/freertos/","title":"FreeRTOS"},{"content":"It\u0026rsquo;s a state feedback just like pole placement, but it\u0026rsquo;s optimal. It\u0026rsquo;s optimal because it minimizes a cost function. why\nLinear: because the system is linear (which means needs to be linearized) Quadratic: because the cost function is quadratic Regulator: because the goal is to stabilize the system Modeling In LQR problems, having a linear model is a must.\nThe linear model might be from a nonlinear model that is linearized around an operating point.\nAnd the building of this nonlinear model is ususally based on the lagrangian dynamics of the system.\nFinite Horizon LQR Why \u0026ldquo;finite horizon\u0026rdquo;? A: because the cost function is defined over a finite time horizon The goal is to reach a desired goal $x_g$ in finite time When the desired goal is constant, the controller is a regulator When the desired goal is time-varying, the controller is a tracker In regulation problems, feedback is enough to stabilize the system In tracking problems, feedforward is needed to stabilize the system The cost (cost-to-go) function is the sum of the state and control cost $$ J = \\frac{1}{2} \\sum_{k=0}^{N-1} [(x_k - x_g)^T Q (x_k - x_g) + u_k^T R u_k] + \\frac{1}{2} (x_N - x_g)^T Q_f (x_N - x_g) $$ $J$: cost function\n$N$: time horizon\n$Q$: state cost matrix\n$R$: control cost matrix$\n$Q_f$: terminal state cost matrix\n$u_k$: control at time k\n$x_k$: state at time k\n$x_g$: desired goal\nCode Example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 import numpy as np from scipy from signal, linalg # 1.39 1.55 20000 25854 1888.6 9.81 0.01 # Parameters for system dynamics lr = 1.39 lf = 1.55 Ca = 20000 Iz = 25854 m = 1888.6 g = 9.81 dt = 0.01 # System dynamics A = np.array([[0, 1, 0, 0], [0, -(2*Ca)/(m*lr), (2*Ca)/m, 0], [0, 0, 0, 1], [0, -(2*Ca*lf - 2*Ca*lr)/(Iz*lf), (2*Ca*lf - 2*Ca*lr)/(Iz*lf), 0]]) B = np.array([[0], [2*Ca/m], [0], [2*Ca*lf/Iz]]) C = np.eye(4) D = np.zeros((4,1)) # generate Q and R according to the size of A and B Q = np.eye(4) R = np.eye(1) # Discretize the system dynamics system_cont = signal.StateSpace(A, B, C, D) system_disc = system_cont.to_discrete(dt) Ad, Bd = system_disc.A, system_disc.B #solve for the P that optimizes the cost function using the discrete algebraic Riccati equation P = linalg.solve_discrete_are(Ad, Bd, Q, R) K = linalg.inv(R + Bd.T @ P @ Bd) @ Bd.T @ P @ Ad e1 = x - x_g # x = [x, x_dot, y, y_dot] just for example u = -K @ e1 All the code above is just to find the optimal control $u$ that minimizes the cost function $J$. The detail implementation of the Riccati equation would be shown below.\nDynamic Programming Known\nThe system dynamics A, B The cost function J The terminal cost $Q_f$ The time horizon N Bruteforce Compute the cost-to-go for all possible states and controls Choose the control that minimizes the cost-to-go = optimize the cost function Computational complex needs a lot of tuning Learning gradient descent the cost function Flexibility in the cost function Riccati Riccati is used to compute the optimal feedback gain matrix $K$ introduce P. what is this P? P is solution to the Riccati equation, it\u0026rsquo;s added to simplify the cost function now $J = x_0^T P x_0 - x_0^T P x_0 + \\int_0^N (x^T Q x + u^T R u) dt$ By doing a trick called completing the square, now we have $$ J = x_0^T P x_0 + \\int_0^N (x^T(A^T P + PA + Q - PBR^{-1}B^T P)x + (u+R^{-1}B^T P x)^T R (u+R^{-1}B^T P x)) dt $$ so now it\u0026rsquo;s easy to see that the optimal control is $u = -R^{-1}B^T P x$ Since $u = -Kx$, we can find K by $K = R^{-1}B^T P$ Now we can find P by solving the Riccati equation The Riccati equation is $A^T P + PA - PBR^{-1}B^T P + Q = 0$\n$P = Q + A^T P A - A^T P (A-BK)$ Since this is a quadratic equation, there would be two possible solutions, one is stable and the other is unstable The stable solution is the one we are looking for. $$ \\frac{\\partial J}{\\partial u} = 0 \\\\ u^* = -[R+B^TPB]^{-1}B^TPA $$$$ R + B^TPB \u003e 0 $$ global minimum\nHow it actually works\n1 2 3 4 5 6 7 8 9 10 P = zeros(n,n,N) K = zeros(m,n,N-1) P[:,:,N] .= Qn # Backward Riccati recursion for k = (N-1):-1:1 K[:,:,k] .= (R + B\u0026#39;*P[:,:,k+1]*B)\\(B\u0026#39;*P[:,:,k+1]*A) P[:,:,k] .= Q + A\u0026#39;*P[:,:,k+1]*(A-B*K[:,:,k]) end Starting from the last time step, compute the optimal feedback gain matrix K and the solution to the Riccati equation P Move backward in time and compute the optimal feedback gain matrix K and the solution to the Riccati equation P Q: In Riccati is only one k is calculated, or is k dynamically changing? A: k is changing, it\u0026rsquo;s a loop that goes from N-1 to 1 Every time step, we compute the optimal feedback gain matrix K and the solution to the Riccati equation P. We would put them in a matrix and do feedback policy for each time step control.\nComparison with Pole Placement Pole placement is a special case of LQR The problem with pole placement is You could only tune the poles and affect the transient response. You can\u0026rsquo;t tune the steady-state error The transition from the response plot to the physcal system is not very translational. Which means while you could tune the pole to make the system more stable, the connection between the pole and the performance is not very clear. In LQR, you could tune the Q and R to directly modify the error and the control effort. Infinite Horizon LQR $$ J = \\sum_{k=0}^{\\infty} [(x_k - x_g)^T Q (x_k - x_g) + u_k^T R u_k] $$ The difference is that in infinite horizon LQR, we don\u0026rsquo;t have a clear terminal state. While in finite LQR, the context is usually to complete a task or to reach a goal, in infinite LQR, the context is usually to design a controller (with a proper K) that stabilizes the system.\n","date":"0001-01-01T00:00:00Z","permalink":"https://lyinandy0917.github.io/p/control/lqr/","title":"LQR Control"},{"content":"Model Predictive Control While LQR works really well with a good model, but in reality there are a couple of things that would prevent us from getting good results:\nimperfect state estimation (sensors are noisy) unmodeled dynamics (What if we don\u0026rsquo;t know the dynamics of the system?) misalignment between the model and the real system (What if the model is not perfect?) actuator constraints Q: what is an actuator?\nA: An actuator is a component of a machine that is responsible for moving and controlling a mechanism or system, for example by opening a valve. In simple terms, it is a \u0026ldquo;mover\u0026rdquo;. All of these above leads to a \u0026ldquo;Sim to Real\u0026rdquo; gap.\nProcess $$ J = \\sum_{i=0}^{N-1} (x(k+i|k) - x_{ref}(k+i))^T Q (x(k+i|k) - x_{ref}(k+i)) + u(k+i|k)^T R u(k+i|k) + (x(k+N|k) - x_{ref}(k+N))^T Q_f (x(k+N|k) - x_{ref}(k+N)) $$ measure the state X at time k. predict the state X at time k+1, k+2, \u0026hellip;, k+N. calculate the control input U at time k, k+1, \u0026hellip;, k+N-1. $$ x(k+1|k) = Ax(k|k) + Bu(k|k) $$ $$ x(k+2|k) = Ax(k+1|k) + Bu(k+1|k) $$ $$ x(k+3|k) = Ax(k+2|k) + Bu(k+2|k) $$ $$ X_k = [I, A, A^2, ..., A^N] X_0 + [B, AB, A^2B, ..., A^{N-1}B] U_k \\\\ = M X_k + C U_k $$ Minimize the cost function J in the horzion (k, k+1, \u0026hellip;, k+N). $$ J = X_k^T Q X_k + U_k^T R U_k + X_{N|k}^T Q_f X_{N|k} $$ From here, we still have two unknowns: X_k and U_k. Plug in $$ X_K = M X_k + C U_k $$\nM and C are known based on A and B.\nAt the end we would have $$ J = X_K^T G X_K + 2U_k^T E U_k + U_k^T H U_k $$ The first term is the initial state and the rest is quadratic programming. $$ G = M^T Q M\\\\ E = M^T Q C\\\\ H = C^T Q C + R\\\\ Q = [Q, 0, ..., 0; 0, Q, ..., 0; ..., 0, 0, Q_f]\\\\ R = [R, 0, ..., 0; 0, R, ..., 0; ..., 0, 0, R]\\\\ $$ Solve the optimization problem. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 % MPC clear all; close all; clc; pkg load control,optim; % System dynamics A = [1 0.1; 0 2]; n = size(A,1); B = [0; 0.5]; p = size(B,2); % Cost function Q = eye(n); Qf = Q; R = eye(p); k = 100; %simulation time X_K = zeros(n,k); X_K(:,1) = [1; 1]; U_K = zeros(p,k); % Prediction horizon N = 10; [E,H] = MPC_cost(A,B,Q,Qf,R,N) for k = 1:k % Solve the optimization problem U_K(:,k) = prediction(X_K(:,k), E, H, N,p); X_K(:,k+1) = A*X_K(:,k) + B*U_K(:,k); end ","date":"0001-01-01T00:00:00Z","permalink":"https://lyinandy0917.github.io/p/mpc/","title":"Model Predictive Control"},{"content":"Neural Networks Regression Classification could also be done with SVMs could also be done with decision trees could also be done with logistic regression So what we had done before (SVMs, decision trees, logistic regression), these are basically a single layer neural network with one percetron.\nThe essential goal in neural networks is optimize the error by adjusting the weights and biases.\nAt the heart of neural network is the perceptron. A neural network is essentially a collection of perceptrons put in series and parallel. Noted that we do not draw the bias in the diagram.\nWith a proper size of neural network, we can approximate any function.\nIn general, what happens in the hiddent neurons is not interpretable and the only thing we know is that they are trying minimize the error.\nExample:\nwe have 3 features $x_1,x_2,x_3$ and we want to predict $y$, there for 3 weights $w_1,w_2,w_3$ and a bias $b$. Then we would have a activation function.\nXOR problem:\n$x_1,x_2$ are the inputs $y$ is the output if both inputs are the same, then the output is 1 if the inputs are different, then the output is 0 Perceptron The perceptron takes some inputs, weights them and fire a result(0/1). It performs a dot product of the weights and the inputs, adds the bias, and then applies an activation function (such as a sigmoid function) to the result. $$ y = f(a) = f(W \\cdot X)$$ $$ a_{1,1} = \\sum_{i=1}^{n} w_{1,i}x_{0,i}$$ It\u0026rsquo;s simply a dot product of the weights and the inputs.\nForward Propagation $$ \\begin{align} \\vec{a_1} \u0026= \\vec{W_1} \\cdot \\vec{x_0} \\\\ \\vec{x_1} \u0026= f(\\vec{a_1}) \\\\ \\vec{a_2} \u0026= \\vec{W_2} \\cdot \\vec{x_1} \\\\ \\vec{x_2} \u0026= f(\\vec{a_2}) = f(\\vec{W_2} \\cdot \\vec{x_1}) \\\\ \\vec{y} \u0026= f(\\vec{W_3} \\cdot \\vec{x_2}) \\\\ \\end{align} $$ where $f$ is the activation function. The chaining of perceptrons is constructed by this chainning effect (chain rule).\nAt the end, you would manually calculate the error and adjust the weights and biases so that the result best matches the ground truth.\nBackpropagation The key idea of neural network is to minimize a loss function through backpropagation.\nSoftmax $$ \\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}}$$Define a neural network in PyTorch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import torch import torch.nn as nn class MLP(nn.Module): def __init__(self): super(MLP, self).__init__() self.layers = nn.Sequential( nn.Linear(3, 4), nn.ReLU(), nn.Linear(4, 2), nn.ReLU(), nn.Linear(2, 1) ) def forward(self, x): return self.layers(x) model = MLP() Even though in the code the bias term is not presented explicitly, it is still there by default (taken into account).\nFor example $$ f(x) = \\frac{1}{1+e^{-k \\cdot x}} $$ where $k$ determines the shape or slope of the sigmoid function.\nActivation Functions Sigmoid\nTanh\nStep\nSoftplus\nswish\nReLU\nsinc\nsoftsign\nLinear (identity): it takes the input and returns the input. This is basically no activation function.\nStep: Squashes the input to 0 or 1. It is not used in hidden layers.\nSigmod $$ f(x) = \\frac{1}{1+e^{-x}}$$$$ f'(x) = f(x)(1-f(x))$$When the gradient goes to zero, it becomes problematic. This is called the vanishing gradient problem.\nThis is Undesirable for hidden layers.WHY?\nWhy do these gradients matter?\nThe only thing we trying to find is the weights and biases that minimize the error. weak gradient slows the learning process. A zero gradient would kill a neuron. $$ \\frac{\\partial y}{\\partial x} = \\frac{\\partial y}{\\partial z} \\frac{\\partial z}{\\partial x}$$Tanh $$ f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$$$ f'(x) = 1 - f(x)^2$$ Data is centered around 0. It is zero centered. It is better than the sigmoid function. Less \u0026ldquo;positive\u0026rdquo; and \u0026ldquo;negative\u0026rdquo; bias. Undesired for middle layers.\nResNet: Residual Network\nIt uses skip connections to avoid the vanishing gradient problem. Q:\nPositive Bias zero centered why gradient matters hwo does gradient contribute to the result the chain rule? What does the activation function do? ReLu Nowadays, you want to always start with ReLU. It is the most popular activation function. But what if the input is negative? The gradient is zero. A solution is to use the leaky ReLU. We are keep the negative part have a minimal slope but not zero. In addition to that, instead of having the slope in the negative part to be a constant, we can make it a learnable parameter. This is called the Parametric ReLU. ","date":"0001-01-01T00:00:00Z","permalink":"https://lyinandy0917.github.io/p/neuralnetwork/","title":"Neural Networks"},{"content":"Newton\u0026rsquo;s Method Problem Setup Given a problem: Here we are going to solve some equality-constrained optimization problems with Newton\u0026rsquo;s method. We are given a problem\n$$ \\begin{align} \\min_x \\quad \u0026 f(x) \\\\ \\text{st} \\quad \u0026 c(x) = 0 \\end{align}$$$$ \\mathcal{L}(x,\\lambda) = f(x) + \\lambda ^T c(x), $$$$\\begin{align} \\nabla_x \\mathcal{L} = \\nabla_x f(x) + \\bigg[ \\frac{\\partial c}{\\partial x}\\bigg] ^T \\lambda \u0026= 0 \\\\ c(x) \u0026= 0 \\end{align}$$Which is just a root-finding problem. To solve this, we are going to solve for a $z = [x^T,\\lambda]^T$ that satisfies these KKT conditions.\nEquality Constrained Optimization Full Newton\u0026rsquo;s Method Lets say we have a curve function $r(z)$ that we want to find the root of. We can use Newton\u0026rsquo;s method to solve for when $r(z) = 0$. To do this, we specify res_fx(z) as $r(z)$, and res_jac_fx(z) as $\\partial r/ \\partial z$. So the decreasing process is described as follows:\n$$ r(z_{k+1}) = r(z_k) + \\bigg[ \\frac{\\partial r}{\\partial z} \\bigg] $$The problem with this is that when the f is approaching zero, the slope or gradient would become really small, leading it to be slow to converge.\nSo we introduce a new variable $\\Delta z$ to the equation, and we have the following:\n$$ r(z_{k+1}) = r(z_k) + \\bigg[ \\frac{\\partial r}{\\partial z} \\bigg] \\Delta z $$Newton\u0026rsquo;s Method with a Linesearch We use Newton\u0026rsquo;s method to solve for when $r(z) = 0$. To do this, we specify res_fx(z) as $r(z)$, and res_jac_fx(z) as $\\partial r/ \\partial z$. To calculate a Newton step, we do the following:\n$$\\Delta z = -\\bigg[ \\frac{\\partial r}{\\partial z} \\bigg]^{-1} r(z_k)$$$$ \\phi(z_k + \\alpha \\Delta z) \u003c \\phi(z_k)$$ Where $\\phi$ is a \u0026ldquo;merit function\u0026rdquo;, or merit_fx(z) in the code.\nThis is called the \u0026ldquo;Armijo rule\u0026rdquo;, and it is a sufficient decrease condition.\nIn my understanding, the Armiho rule is guranteeing that the line is decreasing.\nQ: What does $\\alpha$ really represent? By by decreasing alpha, what is the effect on the line search? A: Basically for newton method, in decreasing, we use the slope of that point to find the next point. So by decreasing alpha, we are decreasing the step size.\n1 2 3 4 5 6 7 8 9 10 function linesearch(z::Vector, Δz::Vector, merit_fx::Function; max_ls_iters = 10)::Float64 α = 1.0 for i = 1:max_ls_iters if merit_fx(z + α * Δz) \u0026lt; merit_fx(z) return α end α /= 2.0 end error(\u0026#34;linesearch failed\u0026#34;) end This ensure a sufficient decrease in the merit function. If the merit function is not decreasing, we would decrease the step size by half and try again. The function fails when no matter how small the step size is, we can\u0026rsquo;t get a sufficient decrease.\nNow we would throw the linesearch function into the newton function. The newton function would calculate the Newton step and then use the linesearch function to find the optimal step size.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 function newtons_method(z0::Vector, res_fx::Function, res_jac_fx::Function, merit_fx::Function; tol = 1e-10, max_iters = 50, verbose = false)::Vector{Vector{Float64}} # - z0, initial guess # - res_fx, residual function # - res_jac_fx, Jacobian of residual function wrt z # - merit_fx, merit function for use in linesearch # optional arguments # - tol, tolerance for convergence. Return when norm(residual)\u0026lt;tol # - max iter, max # of iterations # return a vector of vectors containing the iterates # the last vector in this vector of vectors should be the approx. solution # return the history of guesses as a vector Z = [z0] # Initialize with the initial guess for i = 1:max_iters r = res_fx(Z[end]) norm_r = norm(r) if verbose println(\u0026#34;iter: $i |r|: $norm_r\u0026#34;) end if norm_r \u0026lt; tol return Z end J = res_jac_fx(Z[end]) Δz = -J \\ r α = linesearch(Z[end], Δz, merit_fx, max_ls_iters = 10) Z = vcat(Z, [Z[end] + α * Δz]) # this ensure that it includes the new guess as it also satisfies the merit function if verbose println(\u0026#34;α: $α\u0026#34;) end end error(\u0026#34;Newton\u0026#39;s method did not converge\u0026#34;) end Next we are going to use the newton method to solve the equality-constrained optimization problem. We are going to use the following functions:\nFirst, let\u0026rsquo;s define the cost function and the constraint function.\n1 2 3 4 5 6 7 8 function cost(x::Vector) Q = [1.65539 2.89376; 2.89376 6.51521]; q = [2;-3] return 0.5*x\u0026#39;*Q*x + q\u0026#39;*x + exp(-1.3*x[1] + 0.3*x[2]^2) end function constraint(x::Vector) norm(x) - 0.5 end Second we would define the constraint Jacobian and the Lagrangian.\n1 2 3 4 5 6 7 8 9 10 function constraint_jacobian(x::Vector)::Matrix # since `constraint` returns a scalar value, ForwardDiff # will only allow us to compute a gradient of this function J = reshape(FD.gradient(constraint, x), 1, 2) # J = FD.gradient(constraint, x) # In general, the gradient is length(x)×1 as a column vector or # 1×length(x) as a row vector—depending on how you need it shaped. # return reshape(J, 1, length(J)) could do this without loss of generality return J end Now we would work to implement the KKT conditions.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 function kkt_conditions(z::Vector)::Vector # TODO: return the KKT conditions x = z[1:2] # x is the first two elements of z λ = z[3] # λ is the third element of z # Since we are solving a equality-constrained optimization problem, the Z would be [x,λ] # TODO: return the stationarity condition for the cost function # and the primal feasibility grad_f = ForwardDiff.gradient(cost, x) grad_g = vec(constraint_jacobian(x)) stationarity = grad_f + λ * grad_g feasibility = constraint(x) #error(\u0026#34;kkt not implemented\u0026#34;) return vcat(stationarity, feasibility) end Because those KKT conditions are your residual vector \\(r(z)\\). In other words:\n$$ \\begin{align} \\nabla_x \\mathcal{L} = \\nabla_x f(x) + \\bigg[ \\frac{\\partial c}{\\partial x}\\bigg] ^T \\lambda \u0026= 0 \\\\ c(x) \u0026= 0 \\end{align} $$ $$ r(z) = \\begin{bmatrix} \\nabla_x f(x) + \\bigg[ \\frac{\\partial c}{\\partial x}\\bigg] ^T \\lambda \\\\ c(x) \\end{bmatrix} = 0 $$$$ r(z_{k+1}) = r(z_k) + \\bigg[ \\frac{\\partial r}{\\partial z} \\bigg] \\Delta z \\\\ \\Delta z = -\\bigg[ \\frac{\\partial r}{\\partial z} \\bigg]^{-1} r(z_k) $$ So you could see we need the derivative of the residual function to compute the newton step $\\Delta z$.\n$$\\begin{bmatrix} \\underbrace{\\tfrac{\\partial}{\\partial x}\\Bigl(\\nabla f(x) + \\Bigl[\\tfrac{\\partial c}{\\partial x}(x)\\Bigr]^T \\,\\lambda\\Bigr)}_{\\text{2×2 block, call it }A} \u0026 \\underbrace{\\tfrac{\\partial}{\\partial \\lambda}\\Bigl(\\nabla f(x) + \\Bigl[\\tfrac{\\partial c}{\\partial x}(x)\\Bigr]^T \\,\\lambda\\Bigr)}_{\\text{2×1 block, call it }b} \\\\[8pt] \\underbrace{\\tfrac{\\partial}{\\partial x}\\,c(x)}_{\\text{1×2 block, call it }c} \u0026 \\underbrace{\\tfrac{\\partial}{\\partial \\lambda}\\,c(x)}_{\\text{(1×1) block, typically }0} \\end{bmatrix}$$ Putting it all together:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 function fn_kkt_jac(z::Vector)::Matrix # TODO: return full Newton Jacobian of kkt conditions wrt z x = z[1:2] λ = z[3] # TODO: return full Newton jacobian with a 1e-3 regularizer # to ensure the matrix is invertible # we need the A, b, c, d A = ForwardDiff.jacobian(z -\u0026gt; kkt_conditions(z)[1:2], z) b = ForwardDiff.jacobian(z -\u0026gt; kkt_conditions(z)[1:2], z)[3] c = ForwardDiff.jacobian(z -\u0026gt; kkt_conditions(z)[3], z)[1:2] d = ForwardDiff.jacobian(z -\u0026gt; kkt_conditions(z)[3], z)[3] J = vcat(hcat(A, b), hcat(c, d) + 1e-3 * I) return J end Now let\u0026rsquo;s make a small modification to the newtons_method function to use the gn_kkt_jac function.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 function gn_kkt_jac(z::Vector)::Matrix # TODO: return Gauss-Newton Jacobian of kkt conditions wrt z x = z[1:2] λ = z[3] # TODO: return Gauss-Newton jacobian with a 1e-3 regularizer #error(\u0026#34;gn_kkt_jac not implemented\u0026#34;) # Compute the Hessian of cost. H_f = FD.hessian(cost, x) # Compute the gradient of the constraint. grad_g = vec(constraint_jacobian(x)) # For the Gauss-Newton approximation we use H_f alone. A = H_f # instead of H_f + λ * H_g. b = grad_g c = grad_g\u0026#39; d = 0.0 # Build the approximate Jacobian. J = [A b; c d] # Add a 1e-3 regularizer to the diagonal. J -= 1e-3 * I(3) return J end When the constraints is almost satisfied, the Gauss-Newton method would be more efficient than the full Newton method.\n","date":"0001-01-01T00:00:00Z","permalink":"https://lyinandy0917.github.io/p/newtons-method/","title":"Newton's Method"},{"content":"Path Finding A* Algorithm A star is a heuristic search algorithm that finds the shortest path between the start node and the end node. What is a heuristic? A heuristic is like a rule of thumb. It could be a guess, an estimate, or a rule that helps you find a solution. An Example Let\u0026rsquo;s say you are a mouse in a maze. The heuristic would be how strong the smell of cheese is. The stronger the smell, the closer you are to the cheese. The heuristic is the smell of the cheese. So that the P would be determined by the distance and the smell of the cheese. The distance is the cost of the path, and the smell of the cheese is the heuristic. So each node would have a heuristic value and between the node are the cost of the path. The A* algorithm would find the path with the minimum cost. Code Example First we need to define some helper functions to calculate the heuristic and the cost of the path.\nWe would start by defining the node class.\nWhat is a node in A star algorithm? A node is a point in the grid. It has a position, a parent, and a cost. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 class Node(object): def __init__(self, pose): self.pose = np.array(pose) self.x = pose[0] self.y = pose[1] self.g_value = 0 # cost from start node to current node self.h_value = 0 # heuristic value self.f_value = 0 # f = g + h self.parent = None def __lt__(self, other): # use to compare two nodes: less than return self.f_value \u0026lt; other.f_value def __eq__(self, other): # use to compare two nodes: equal return (self.pose == other.pose).all() Now we could start to craft the A* class. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 class AStar(object): def __init__(self, map_path): self.map_path = map_path self.map = self.load_map(self.map_path).astype(int) print(self.map) self.resolution = 0.05 self.y_dim = self.map.shape[0] self.x_dim =self.map.shape[1] print(f\u0026#39;map size ({self.x_dim}, {self.y_dim})\u0026#39;) def load_map(self, path): return np.load(path) def reset_map(self): self.map = self.load_map(self.map_path) The load_map function is used to load the map from the file. The reset_map function is used to reset the map to the original state. why do we need to reset the map? Because we need to find the path multiple times, and we need to reset the map to the original state every time we find a new path.\nNow we would define the important parts of the euclidean distance and the cost of the path. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 class AStar(object): \u0026#39;\u0026#39;\u0026#39; \u0026#39;\u0026#39;\u0026#39; def euclidean_distance(self, start, end): return np.linalg.norm(start.pose - end.pose) # this is euclidean distance therefore it\u0026#39;s the straight line distance def get_successor(self, node): \u0026#34;\u0026#34;\u0026#34; :param node: A Node data structure :return: a list of Nodes containing successors of current Node \u0026#34;\u0026#34;\u0026#34; successor_list = [] x,y = node.pose # Get x, y coordinates of the current node pose_list = [[x+1, y+1], [x, y+1], [x-1, y+1], [x-1, y], [x-1, y-1], [x, y-1], [x+1, y-1], [x+1, y]] # Pose list contains 8 neighbors of the current node for pose_ in pose_list: x_, y_ = pose_ if 0 \u0026lt;= x_ \u0026lt; self.y_dim and 0 \u0026lt;= y_ \u0026lt; self.x_dim and self.map[x_, y_] == 0: # Eliminate nodes that are out of bound, and nodes that are obstacles if the value is 0 then it\u0026#39;s not an obstacle self.map[x_, y_] = -1 successor_list.append(Node(pose_)) return successor_list Now we would define the calculate the path function. Before that, there are two useful functions: heappush and heappop. heappush is used to push the node into the heap. heappop is used to pop the node with the lowest f value from the heap. The heap is a data structure that allows you to push and pop the node with the lowest f value in O(1) time. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class AStar(object): \u0026#39;\u0026#39;\u0026#39; \u0026#39;\u0026#39;\u0026#39; def calculate_path(self, node): \u0026#34;\u0026#34;\u0026#34; :param node: A Node data structure :return: a list with shape (n, 2) containing n path point \u0026#34;\u0026#34;\u0026#34; path_ind = [] path_ind.append(node.pose.tolist()) current = node while current.parent: current = current.parent path_ind.append(current.pose.tolist()) path_ind.reverse() print(f\u0026#39;path length {len(path_ind)}\u0026#39;) # the number of nodes in the path path = list(path_ind) return path It would return the path in the form of a list of nodes.\nWe would define the plan function. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 class AStar(object): \u0026#39;\u0026#39;\u0026#39; \u0026#39;\u0026#39;\u0026#39; def plan(self, start_ind, goal_ind): \u0026#34;\u0026#34;\u0026#34; TODO: Fill in the missing lines in the plan function @param start_ind : [x, y] represents coordinates in webots world @param goal_ind : [x, y] represents coordinates in webots world @return path : a list with shape (n, 2) containing n path point \u0026#34;\u0026#34;\u0026#34; # initialize start node and goal node class start_node = Node(start_ind) goal_node = Node(goal_ind) \u0026#34;\u0026#34;\u0026#34; TODO: calculate h and f value of start_node (1) h can be computed by calling the heuristic method (2) f = g + h \u0026#34;\u0026#34;\u0026#34; # Calculate initial h and f values for start_node start_node.h_value = self.heuristic(start_node, goal_node) # from start to goal start_node.f_value = start_node.g_value + start_node.h_value # Reset map and initialize open and closed lists self.reset_map() open_list = [] closed_list = [] heappush(open_list, start_node) #why do we need to push the start node into the open list? Because we need to start from the start node. but isn\u0026#39;t open list for unvisited nodes? Yes, but we need to start from the start node. # Reset map self.reset_map() # Initially, only the start node is known. # This is usually implemented as a min-heap or priority queue rather than a hash-set. # Please refer to https://docs.python.org/3/library/heapq.html for more details about heap data structure open_list = [] # open list is a list of nodes that are not yet visited closed_list = np.array([]) heappush(open_list, start_node) # while open_list is not empty while len(open_list): \u0026#34;\u0026#34;\u0026#34; TODO: get the current node and add it to the closed list \u0026#34;\u0026#34;\u0026#34; # Current is the node in open_list that has the lowest f value # This operation can occur in O(1) time if open_list is a min-heap or a priority queue # Get the node with the lowest f value from open_list current = heappop(open_list) closed_list = np.append(closed_list, current) self.map[current.x, current.y] = -1 # if current is goal_node: calculate the path by passing through the current node # exit the loop by returning the path if current == goal_node: print(\u0026#39;reach goal\u0026#39;) return self.calculate_path(current) for successor in self.get_successor(current): \u0026#34;\u0026#34;\u0026#34; TODO: 1. pass current node as parent of successor node 2. calculate g, h, and f value of successor node (1) d(current, successor) is the weight of the edge from current to successor (2) g(successor) = g(current) + d(current, successor) (3) h(successor) can be computed by calling the heuristic method (4) f(successor) = g(successor) + h(successor) \u0026#34;\u0026#34;\u0026#34; # Set current node as the parent of the successor successor.parent = current # Calculate g, h, and f values for the successor distance = np.linalg.norm(successor.pose - current.pose) successor.g_value = current.g_value + distance successor.h_value = self.heuristic(successor, goal_node) successor.f_value = successor.g_value + successor.h_value heappush(open_list, successor) # If the loop is exited without return any path # Path is not found print(\u0026#39;path not found\u0026#39;) return None ","date":"0001-01-01T00:00:00Z","permalink":"https://lyinandy0917.github.io/p/path-finding/","title":"Path Finding"},{"content":"PID Control ","date":"0001-01-01T00:00:00Z","permalink":"https://lyinandy0917.github.io/p/pid-control/","title":"PID Control"},{"content":"ROS Intro Q: why do we need ROS?\nA: ROS is a framework for writing robot software. It is a collection of tools, libraries, and conventions that aim to simplify the task of creating complex. Ros has provided a high efficient way to communicate between modules, algorithms and drivers.\nOperating System - Linux Q: Why linux?\nA: ROS has been very mature on Linux. It is the most popular operating system for robots.\nOperating system manages the hardware and software resources of the system. It is the most fundamental software that runs on a computer. Linux is the most popular operating system for robots. 一般我们说的linux是指他的发行版 Installation of Linux Download the Ubuntu LTS version from the official website (An iso file). Download VMware Workstation Pro from the official website. Open VMware Workstation Pro and create a new virtual machine. Assign \u0026gt;2GB RAM and \u0026gt;20GB storage. Choose the installed ISO file. Install Ubuntu on the virtual machine. Follow the instructions to install Ubuntu. Download the VMware tools Open the terminal sudo apt upgrade sudo apt install open-vm-tools-desktop -y sudo reboot Simple Commands ls - list files and directories cd - change directory pwd - print working directory mkdir - make directory rm - remove file or directory rmdir - remove directory To open the terminal, press Ctrl + Alt + T \u0026ndash;help - to get help on a command mv - move files or directories cp - copy files or directories touch - create a file sudo - run a command as superuser Install g++ and python sudo apt-get install g++\nTo compile a C++ program, use g++ filename.cpp -o outputname\nPython3 is already installed on Ubuntu.\nInstall ROS2 Set Locale sudo apt update sudo apt install locales sudo locale-gen en_US en_US.UTF-8 sudo update-locale LC_ALL=en_US.UTF-8 LANG=en_US.UTF-8 export LANG=en_US.UTF-8 locale Enable ROS2 repository sudo apt install software-properties-common sudo add-apt-repository universe sudo apt update \u0026amp;\u0026amp; sudo apt install curl -y curl -s https://raw.githubusercontent.com/ros/rosdistro/master/ros.key -o /usr/share/keyrings/ros-archive-keyring.gpg echo \u0026quot;deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/ros-archive-keyring.gpg] http://packages.ros.org/ros2/ubuntu $(. /etc/os-release; echo \u0026quot;$UBUNTU_CODENAME\u0026quot;) main\u0026quot; | sudo tee /etc/apt/sources.list.d/ros2.list \u0026gt; /dev/null Install development tools and ROS tools sudo apt update \u0026amp;\u0026amp; sudo apt install ros-dev-tools Install ROS2 sudo apt update \u0026amp;\u0026amp; sudo apt upgrade sudo apt install ros-jazzy-desktop sudo apt install ros-jazzy-ros-base\nCommon Bugs refer to BUG FIX ROS2 Commands ros2 run package_name executable_name - run a ROS2 node ros2 node list - list all the nodes ros2 node info /node_name - get information about a node ros2 topic list - list all the topics ros2 topic echo /topic_name - echo the messages on a topic ros2 topic pub --rate 1 /target/cmd_vel geometry_msgs/Twist '{linear: {x: 0.1}, angular: {z: 0.1}}' - publish a message on a topic ros2 bag record /target/cmd_vel - record messages on a topic ros2 bag play /target/cmd_vel - play messages from a bag file This would be useful for record a behavior and replay it later for other experiments. VSCode Install VSCode Install some extensions C/C++ Python ROS Jupyter Cmake Gazebo Gazebo is a 3D simulator that allows you to test your robot in a virtual environment. sudo apt-get install ros-${ROS_DISTRO}-ros-gz Refer to Gazebo Installation for corresponding ROS ","date":"0001-01-01T00:00:00Z","image":"https://lyinandy0917.github.io/p/ros/ROS_hu_22c6bf6b6cff39d2.png","permalink":"https://lyinandy0917.github.io/p/ros/","title":"ROS"},{"content":"SVM Direct Approach: finding the hyperplane that best separates the classes (furthest from the nearest data points)\n$$ margin (W,b) = min_{i} \\frac{y^{(i)}(W^T x^{(i)} + b)}{||W||} $$","date":"0001-01-01T00:00:00Z","permalink":"https://lyinandy0917.github.io/p/svm/","title":"SVM"},{"content":"Unconstrained Optimization $$ \\min_x \\quad f(x) $$ Assumption:\n$f(x)$ is twice differentiable $f(x)$ is convex. The hessian of the function is positive semi-definite. (This means the function is bowl-shaped) optimal solution $x^*$ exists Optimal condition $\\nabla f(x^*) = 0$. This mean the gradient of the function at the optimal solution is zero. Convex Quadratic Function $$ \\begin{align} \\min_x \\quad \u0026 \\frac{1}{2} x^T P x + q^T x + r \\\\ \\quad \u0026 \\nabla f(x)= P x + q = 0 \\end{align} $$ P is the hessian of the function q is the gradient of the function r is the constant term Notation Explained\nP is the hessian of the function q is the gradient of the function r is the constant term $$ f(x) = f(x_0) + \\nabla f(x_0)^T (x - x_0) + \\frac{1}{2} (x - x_0)^T \\nabla^2 f(x_0) (x - x_0) $$We could solve this problem by setting the gradient to zero. Q: Why do we set the gradient to zero? A: Because the gradient is zero at the optimal solution.\nMethod: Gradient Descent $$ x_{k+1} = x_k + t_k \\Delta x_k $$$$ \\Delta x_k = - \\nabla f(x_k) $$ choose a starting point $x_0$ repeat until convergence: calculate the gradient $\\nabla f(x_k)$ line search: find the step size $t_k$ that minimizes $f(x_k + t_k \\Delta x_k)$ update $x_{k+1} = x_k + t_k \\Delta x_k$ Stop when L2 norm of the gradient is less than a threshold. $$ \\nabla f(x_k) \u003c \\text{threshold} $$ ","date":"0001-01-01T00:00:00Z","permalink":"https://lyinandy0917.github.io/p/unconstrained-optimization/","title":"Unconstrained Optimization"}]